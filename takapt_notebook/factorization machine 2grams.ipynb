{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "4b29202d-9d1b-7272-13e6-c93d408ce5fe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takaya/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.optimize import minimize\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import multiprocessing\n",
    "import difflib\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stops = stopwords.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    'clean_q1_lemmatized',\n",
    "    'clean_q2_lemmatized',\n",
    "#     'clean_q1_lemmatized_stem',\n",
    "#     'clean_q2_lemmatized_stem'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(t, features):\n",
    "    data = pd.read_csv('../input/{}.csv'.format(t))\n",
    "    for feature in features:\n",
    "        data = pd.merge(data, pd.read_pickle('../feature/{}_{}.pkl'.format(t, feature)))\n",
    "    data.fillna(0.0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = read_data('train', features)#[:1000]\n",
    "test = read_data('test', features)#[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_ngrams(a, n):\n",
    "    return [' '.join(a[i:i + n]) for i in range(len(a) - n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_dict_features():\n",
    "    from itertools import chain\n",
    "    from collections import Counter\n",
    "    counter = Counter(chain.from_iterable(\n",
    "        [list_ngrams(words, N) for words in train.clean_q1_lemmatized.tolist()] + \n",
    "        [list_ngrams(words, N) for words in train.clean_q2_lemmatized.tolist()] + \n",
    "        [list_ngrams(words, N) for words in test.clean_q1_lemmatized.tolist()] + \n",
    "        [list_ngrams(words, N) for words in test.clean_q2_lemmatized.tolist()]\n",
    "    ))\n",
    "    \n",
    "    word_set = set(word for word, freq in counter.items() if freq >= 4)\n",
    "    return ['q1_' + word for word in word_set] + ['q2_' + word for word in word_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_dict_list(corpus):\n",
    "    return [{word: 1 for word in words} for words in corpus]\n",
    "\n",
    "def map_q1q2_dict(row):\n",
    "    q1 = ['q1_' + word for word in list_ngrams(row.clean_q1_lemmatized, N)]\n",
    "    q2 = ['q2_' + word for word in list_ngrams(row.clean_q2_lemmatized, N)]\n",
    "    return {word: 1 for word in q1 + q2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
       "        sparse=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = DictVectorizer(sparse=True)\n",
    "vec.fit([{word: 1 for word in make_dict_features()}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2287628"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_q1q2 = vec.transform(train.apply(map_q1q2_dict, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FMClassification(init_stdev=0.02, l2_reg=None, l2_reg_V=1, l2_reg_w=1,\n",
       "         n_iter=1000, random_state=123, rank=100, step_size=0.1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastFM import sgd\n",
    "\n",
    "fm = sgd.FMClassification(n_iter=1000, init_stdev=0.02, l2_reg_w=1, l2_reg_V=1, rank=100, step_size=0.1)\n",
    "fm.fit(train_q1q2, np.where(train.is_duplicate == 0, -1, +1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['fm_2grams'] = fm.predict_proba(train_q1q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['fm_2grams'] = fm.predict_proba(vec.transform(test.apply(map_q1q2_dict, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_save = [\n",
    "    'fm_2grams',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20.0, 5.0)\n",
    "\n",
    "for feature in features_to_save:\n",
    "    plt.title(feature)\n",
    "    plt.hist(train[feature][train['is_duplicate'] == 0], bins=20, normed=True, label='0')\n",
    "    plt.hist(train[feature][train['is_duplicate'] == 1], bins=20, normed=True, label='1', alpha=0.7)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import util\n",
    "util.save_feature(train, 'train', features_to_save, 'id')\n",
    "util.save_feature(test, 'test', features_to_save, 'test_id')"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 43,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
